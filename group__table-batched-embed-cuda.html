<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>fbgemm_gpu: CUDA Operators</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">fbgemm_gpu
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',false);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="doc-content">
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">CUDA Operators</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga6ad338e73d42103b8604528dcffac76c" id="r_ga6ad338e73d42103b8604528dcffac76c"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; at::Tensor, at::Tensor, std::optional&lt; at::Tensor &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga6ad338e73d42103b8604528dcffac76c">get_unique_indices_cuda</a> (const at::Tensor &amp;linear_indices, const int64_t max_indices, const bool compute_count)</td></tr>
<tr class="separator:ga6ad338e73d42103b8604528dcffac76c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gadbd88ffa739ccb8b0a73d891dcb2b9dc" id="r_gadbd88ffa739ccb8b0a73d891dcb2b9dc"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; at::Tensor, at::Tensor, std::optional&lt; at::Tensor &gt;, std::optional&lt; at::Tensor &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gadbd88ffa739ccb8b0a73d891dcb2b9dc">get_unique_indices_with_inverse_cuda</a> (const at::Tensor &amp;linear_indices, const int64_t max_indices, const bool compute_count, const bool compute_inverse_indices)</td></tr>
<tr class="separator:gadbd88ffa739ccb8b0a73d891dcb2b9dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga30c14548acd520585b982112794e0bb0" id="r_ga30c14548acd520585b982112794e0bb0"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; at::Tensor, at::Tensor, std::optional&lt; at::Tensor &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga30c14548acd520585b982112794e0bb0">lru_cache_find_uncached_cuda</a> (at::Tensor unique_indices, at::Tensor unique_indices_length, int64_t max_indices, at::Tensor lxu_cache_state, int64_t time_stamp, at::Tensor lru_state, bool gather_cache_stats, at::Tensor uvm_cache_stats, bool lock_cache_line, at::Tensor lxu_cache_locking_counter, const bool compute_inverse_indices)</td></tr>
<tr class="separator:ga30c14548acd520585b982112794e0bb0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga920da453c443675fc7fbc9d68e272a61" id="r_ga920da453c443675fc7fbc9d68e272a61"><td class="memItemLeft" align="right" valign="top">int64_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga920da453c443675fc7fbc9d68e272a61">host_lxu_cache_slot</a> (int64_t h_in, int64_t C)</td></tr>
<tr class="separator:ga920da453c443675fc7fbc9d68e272a61"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad29a0ca7252a6e080c26c3872923b742" id="r_gad29a0ca7252a6e080c26c3872923b742"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gad29a0ca7252a6e080c26c3872923b742">linearize_cache_indices_cuda</a> (const at::Tensor &amp;cache_hash_size_cumsum, const at::Tensor &amp;indices, const at::Tensor &amp;offsets, const std::optional&lt; at::Tensor &gt; &amp;B_offsets, const int64_t max_B, const int64_t indices_base_offset)</td></tr>
<tr class="separator:gad29a0ca7252a6e080c26c3872923b742"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6eed85d3e9b5dbef8a753bb81c2d6e05" id="r_ga6eed85d3e9b5dbef8a753bb81c2d6e05"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga6eed85d3e9b5dbef8a753bb81c2d6e05">linearize_cache_indices_from_row_idx_cuda</a> (at::Tensor cache_hash_size_cumsum, at::Tensor update_table_indices, at::Tensor update_row_indices)</td></tr>
<tr class="separator:ga6eed85d3e9b5dbef8a753bb81c2d6e05"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga588c4e7ce44cac1a072e708a2fd52d16" id="r_ga588c4e7ce44cac1a072e708a2fd52d16"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga588c4e7ce44cac1a072e708a2fd52d16">direct_mapped_lxu_cache_lookup_cuda</a> (at::Tensor linear_cache_indices, at::Tensor lxu_cache_state, int64_t invalid_index, bool gather_cache_stats, std::optional&lt; at::Tensor &gt; uvm_cache_stats)</td></tr>
<tr class="separator:ga588c4e7ce44cac1a072e708a2fd52d16"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2b055aeb5bf2d99bfb4351271764cab1" id="r_ga2b055aeb5bf2d99bfb4351271764cab1"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga2b055aeb5bf2d99bfb4351271764cab1">lxu_cache_flush_cuda</a> (at::Tensor uvm_weights, at::Tensor cache_hash_size_cumsum, at::Tensor cache_index_table_map, at::Tensor weights_offsets, at::Tensor D_offsets, int64_t total_D, at::Tensor lxu_cache_state, at::Tensor lxu_cache_weights, bool stochastic_rounding)</td></tr>
<tr class="separator:ga2b055aeb5bf2d99bfb4351271764cab1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaeaf8f13290f0fe389fefa3fc2a944311" id="r_gaeaf8f13290f0fe389fefa3fc2a944311"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gaeaf8f13290f0fe389fefa3fc2a944311">lxu_cache_locking_counter_decrement_cuda</a> (at::Tensor lxu_cache_locking_counter, at::Tensor lxu_cache_locations)</td></tr>
<tr class="separator:gaeaf8f13290f0fe389fefa3fc2a944311"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga3ec3ec9faca349d5156cc9d0011f33b8" id="r_ga3ec3ec9faca349d5156cc9d0011f33b8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga3ec3ec9faca349d5156cc9d0011f33b8">lxu_cache_locations_update_cuda</a> (at::Tensor lxu_cache_locations, at::Tensor lxu_cache_locations_new, std::optional&lt; at::Tensor &gt; num_uniq_cache_indices)</td></tr>
<tr class="separator:ga3ec3ec9faca349d5156cc9d0011f33b8"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>The following are CUDA Operators </p>
<h2 class="groupheader">Function Documentation</h2>
<a id="ga588c4e7ce44cac1a072e708a2fd52d16" name="ga588c4e7ce44cac1a072e708a2fd52d16"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga588c4e7ce44cac1a072e708a2fd52d16">&#9670;&#160;</a></span>direct_mapped_lxu_cache_lookup_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor direct_mapped_lxu_cache_lookup_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>linear_cache_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>invalid_index</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>gather_cache_stats</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::optional&lt; at::Tensor &gt;</td>          <td class="paramname"><span class="paramname"><em>uvm_cache_stats</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>LRU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from <code>weights</code>, and insert them into the cache at timestep <code>time_stamp</code>. void lru_cache_populate_cuda(
    at::Tensor weights,
    at::Tensor hash_size_cumsum,
    int64_t total_cache_hash_size,
    at::Tensor cache_index_table_map,
    at::Tensor weights_offsets,
    at::Tensor D_offsets,
    at::Tensor linear_cache_indices,
    at::Tensor lxu_cache_state,
    at::Tensor lxu_cache_weights,
    int64_t time_stamp,
    at::Tensor lru_state,
    bool stochastic_rounding,
    bool gather_cache_stats,
    std::optional&lt;at::Tensor&gt; uvm_cache_stats,
    bool lock_cache_line,
    std::optional&lt;at::Tensor&gt; lxu_cache_locking_counter);</p>
<p>/ / LRU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from /<code>weights</code>, and insert them into the cache at timestep <code>time_stamp</code>. / weights and lxu_cache_weights have "uint8_t" byte elements void lru_cache_populate_byte_cuda(
    at::Tensor weights,
    at::Tensor hash_size_cumsum,
    int64_t total_cache_hash_size,
    at::Tensor cache_index_table_map,
    at::Tensor weights_offsets,
    at::Tensor weights_tys,
    at::Tensor D_offsets,
    at::Tensor linear_cache_indices,
    at::Tensor lxu_cache_state,
    at::Tensor lxu_cache_weights,
    int64_t time_stamp,
    at::Tensor lru_state,
    int64_t row_alignment,
    bool gather_cache_stats,
    std::optional&lt;at::Tensor&gt; uvm_cache_stats);</p>
<p>/ / Direct-mapped (assoc=1) variant of lru_cache_populate_byte_cuda void direct_mapped_lru_cache_populate_byte_cuda(
    at::Tensor weights,
    at::Tensor hash_size_cumsum,
    int64_t total_cache_hash_size,
    at::Tensor cache_index_table_map,
    at::Tensor weights_offsets,
    at::Tensor weights_tys,
    at::Tensor D_offsets,
    at::Tensor linear_cache_indices,
    at::Tensor lxu_cache_state,
    at::Tensor lxu_cache_weights,
    int64_t time_stamp,
    at::Tensor lru_state,
    at::Tensor lxu_cache_miss_timestamp,
    int64_t row_alignment,
    bool gather_cache_stats,
    std::optional&lt;at::Tensor&gt; uvm_cache_stats);</p>
<p>/ / LFU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from /<code>weights</code>, and insert them into the cache. void lfu_cache_populate_cuda(
    at::Tensor weights,
    at::Tensor cache_hash_size_cumsum,
    int64_t total_cache_hash_size,
    at::Tensor cache_index_table_map,
    at::Tensor weights_offsets,
    at::Tensor D_offsets,
    at::Tensor linear_cache_indices,
    at::Tensor lxu_cache_state,
    at::Tensor lxu_cache_weights,
    at::Tensor lfu_state,
    bool stochastic_rounding);</p>
<p>/ / LFU cache: fetch the rows corresponding to <code>linear_cache_indices</code> from /<code>weights</code>, and insert them into the cache. / weights and lxu_cache_weights have "uint8_t" byte elements void lfu_cache_populate_byte_cuda(
    at::Tensor weights,
    at::Tensor cache_hash_size_cumsum,
    int64_t total_cache_hash_size,
    at::Tensor cache_index_table_map,
    at::Tensor weights_offsets,
    at::Tensor weights_tys,
    at::Tensor D_offsets,
    at::Tensor linear_cache_indices,
    at::Tensor lxu_cache_state,
    at::Tensor lxu_cache_weights,
    at::Tensor lfu_state,
    int64_t row_alignment);</p>
<p>/ / Lookup the LRU/LFU cache: find the cache weights location for all indices. / Look up the slots in the cache corresponding to <code>linear_cache_indices</code>, with / a sentinel value for missing. at::Tensor lxu_cache_lookup_cuda(
    at::Tensor linear_cache_indices,
    at::Tensor lxu_cache_state,
    int64_t invalid_index,
    bool gather_cache_stats,
    std::optional&lt;at::Tensor&gt; uvm_cache_stats,
    std::optional&lt;at::Tensor&gt; num_uniq_cache_indices,
    std::optional&lt;at::Tensor&gt; lxu_cache_locations_output);</p>
<p>at::Tensor emulate_cache_miss(
    at::Tensor lxu_cache_locations,
    const int64_t enforced_misses_per_256,
    const bool gather_cache_stats,
    at::Tensor uvm_cache_stats);</p>
<p>/ / Lookup the LRU/LFU cache: find the cache weights location for all indices. / Look up the slots in the cache corresponding to <code>linear_cache_indices</code>, with a sentinel value for missing. </p>

</div>
</div>
<a id="ga6ad338e73d42103b8604528dcffac76c" name="ga6ad338e73d42103b8604528dcffac76c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga6ad338e73d42103b8604528dcffac76c">&#9670;&#160;</a></span>get_unique_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; at::Tensor, at::Tensor, std::optional&lt; at::Tensor &gt; &gt; get_unique_indices_cuda </td>
          <td>(</td>
          <td class="paramtype">const at::Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>linear_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>max_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>compute_count</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Deduplicate indices. </p>

</div>
</div>
<a id="gadbd88ffa739ccb8b0a73d891dcb2b9dc" name="gadbd88ffa739ccb8b0a73d891dcb2b9dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gadbd88ffa739ccb8b0a73d891dcb2b9dc">&#9670;&#160;</a></span>get_unique_indices_with_inverse_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; at::Tensor, at::Tensor, std::optional&lt; at::Tensor &gt;, std::optional&lt; at::Tensor &gt; &gt; get_unique_indices_with_inverse_cuda </td>
          <td>(</td>
          <td class="paramtype">const at::Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>linear_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>max_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>compute_count</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>compute_inverse_indices</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Deduplicate indices. </p>

</div>
</div>
<a id="ga920da453c443675fc7fbc9d68e272a61" name="ga920da453c443675fc7fbc9d68e272a61"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga920da453c443675fc7fbc9d68e272a61">&#9670;&#160;</a></span>host_lxu_cache_slot()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int64_t host_lxu_cache_slot </td>
          <td>(</td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>h_in</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>C</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Map index to cache_set. h_in: linear_indices; C: #cache_sets. </p>

</div>
</div>
<a id="gad29a0ca7252a6e080c26c3872923b742" name="gad29a0ca7252a6e080c26c3872923b742"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gad29a0ca7252a6e080c26c3872923b742">&#9670;&#160;</a></span>linearize_cache_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor linearize_cache_indices_cuda </td>
          <td>(</td>
          <td class="paramtype">const at::Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const at::Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const at::Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::optional&lt; at::Tensor &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>B_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>max_B</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>indices_base_offset</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Linearize the indices of all tables to make it be unique </p>

</div>
</div>
<a id="ga6eed85d3e9b5dbef8a753bb81c2d6e05" name="ga6eed85d3e9b5dbef8a753bb81c2d6e05"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga6eed85d3e9b5dbef8a753bb81c2d6e05">&#9670;&#160;</a></span>linearize_cache_indices_from_row_idx_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor linearize_cache_indices_from_row_idx_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>update_table_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>update_row_indices</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Linearize the indices of all tables to make it be unique. Note the update_table_indices and update_row_indices are from the row indices format for inplace update. </p>

</div>
</div>
<a id="ga30c14548acd520585b982112794e0bb0" name="ga30c14548acd520585b982112794e0bb0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga30c14548acd520585b982112794e0bb0">&#9670;&#160;</a></span>lru_cache_find_uncached_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; at::Tensor, at::Tensor, std::optional&lt; at::Tensor &gt; &gt; lru_cache_find_uncached_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>unique_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>unique_indices_length</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>max_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>time_stamp</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lru_state</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>gather_cache_stats</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>uvm_cache_stats</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>lock_cache_line</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locking_counter</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>compute_inverse_indices</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Lookup LRU cache to find uncached indices, and then sort them based on the set. </p>

</div>
</div>
<a id="ga2b055aeb5bf2d99bfb4351271764cab1" name="ga2b055aeb5bf2d99bfb4351271764cab1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga2b055aeb5bf2d99bfb4351271764cab1">&#9670;&#160;</a></span>lxu_cache_flush_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lxu_cache_flush_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>uvm_weights</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>cache_hash_size_cumsum</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>cache_index_table_map</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>D_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>total_D</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_state</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>stochastic_rounding</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Flush the cache: store the weights from the cache to the backing storage. </p>

</div>
</div>
<a id="ga3ec3ec9faca349d5156cc9d0011f33b8" name="ga3ec3ec9faca349d5156cc9d0011f33b8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga3ec3ec9faca349d5156cc9d0011f33b8">&#9670;&#160;</a></span>lxu_cache_locations_update_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lxu_cache_locations_update_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations_new</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::optional&lt; at::Tensor &gt;</td>          <td class="paramname"><span class="paramname"><em>num_uniq_cache_indices</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Inplace update lxu_cache_locations to the new one should only update if lxu_cache_locations[i] == -1 and lxu_cache_locations_new[i] &gt;= 0 </p>

</div>
</div>
<a id="gaeaf8f13290f0fe389fefa3fc2a944311" name="gaeaf8f13290f0fe389fefa3fc2a944311"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaeaf8f13290f0fe389fefa3fc2a944311">&#9670;&#160;</a></span>lxu_cache_locking_counter_decrement_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void lxu_cache_locking_counter_decrement_cuda </td>
          <td>(</td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locking_counter</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Decrement the LRU/LFU cache counter based on lxu_cache_locations. </p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2
</small></address>
</div><!-- doc-content -->
</body>
</html>
